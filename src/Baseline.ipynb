{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.utils import weight_norm\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import pdb\n",
    "from logger import Logger\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "num_channels = 3\n",
    "num_classes = 2\n",
    "num_epochs = 300\n",
    "image_size = 32\n",
    "batch_size = 64\n",
    "epsilon = 1e-8 # used to avoid NAN loss\n",
    "logger = Logger('./logs')\n",
    "\n",
    "# Initialize parameters\n",
    "lr = 1e-5\n",
    "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam: decay of first order momentum of gradient\n",
    "\n",
    "model_path ='./baseline.tar'\n",
    "image_dir = 'baseline_images'\n",
    "\n",
    "os.makedirs(image_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "class TCGADataset(Dataset):\n",
    "    def __init__(self, image_size, split):\n",
    "        self.split = split\n",
    "        self.tcga_dataset = self._create_dataset(image_size, split)\n",
    "        self.patches, self.labels = self.tcga_dataset\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "        ])\n",
    "        \n",
    "    def _create_dataset(self, image_size, split):\n",
    "        data_dir = '../dataset/patch_data'\n",
    "        if self.split == 'train':\n",
    "            data_dir = os.path.join(data_dir, 'train')\n",
    "        else:\n",
    "            data_dir = os.path.join(data_dir, 'dev')\n",
    "            \n",
    "        all_files = ['5.npz', '6.npz', '7.npz', '8.npz', '9.npz', '10.npz'] #os.listdir(data_dir)\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        # Iterate over all files\n",
    "        for file in all_files:\n",
    "            if '.npz' not in file:\n",
    "                continue\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            data = np.load(file_path)\n",
    "            X = data['arr_0']\n",
    "            y = data['arr_1']\n",
    "            images.append(X)\n",
    "            labels.append(y)\n",
    "            \n",
    "        images = np.concatenate(images)\n",
    "        labels = np.concatenate(labels)\n",
    "        labels = np.asarray([1 if x in [330.0,331.0] else 0 for x in labels])\n",
    "        \n",
    "        # Balance dataset\n",
    "        cancer = np.count_nonzero(labels)\n",
    "        noncancer = (labels.shape[0]-cancer)\n",
    "        minimum = min(cancer,noncancer)\n",
    "        sample_idxs_cancer = random.sample(list(np.where(labels == 1)[0]), minimum)\n",
    "        sample_idxs_nocancer = random.sample(list(np.where(labels == 0)[0]), minimum)\n",
    "        \n",
    "        new_idxs = []\n",
    "        new_idxs.extend(sample_idxs_cancer)\n",
    "        new_idxs.extend(sample_idxs_nocancer)\n",
    "        random.shuffle(new_idxs)\n",
    "        images = images[new_idxs]\n",
    "        labels = labels[new_idxs]\n",
    "        \n",
    "        return images, labels\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.patches[idx], self.labels[idx]\n",
    "        return self.transform(Image.fromarray(data)), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloaders\n",
    "def get_loader(image_size, batch_size):\n",
    "    num_workers = 2\n",
    "    tcga_train = TCGADataset(image_size=image_size, split='train')\n",
    "#     tcga_test = TCGADataset(image_size=image_size, split='test')\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=tcga_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "#     test_loader = DataLoader(\n",
    "#         dataset=tcga_test,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=True\n",
    "#         #num_workers=num_workers\n",
    "#     )\n",
    "\n",
    "    return train_loader#, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-2-9e66dd1fd7f0>(37)train_load()\n",
      "-> print(\"\\nLoaded\", len(images), \"patches \")\n",
      "(Pdb) c\n",
      "\n",
      "Loaded 41000 patches \n",
      "> <ipython-input-2-9e66dd1fd7f0>(37)train_load()\n",
      "-> print(\"\\nLoaded\", len(images), \"patches \")\n",
      "(Pdb) c\n",
      "\n",
      "Loaded 2000 patches \n"
     ]
    }
   ],
   "source": [
    "# # Load Data\n",
    "# def train_load(path, parts,mode='train'):\n",
    "    \n",
    "#     images = []\n",
    "#     labels = []\n",
    "\n",
    "#     for p in parts:\n",
    "#         npz = np.load(os.path.join(path, str(p) + \".npz\"))\n",
    "\n",
    "#         images.append(npz['arr_0'])\n",
    "#         labels.append(npz['arr_1'])\n",
    "        \n",
    "\n",
    "#     images = np.concatenate(images)\n",
    "#     labels = np.concatenate(labels)\n",
    "#     labels = np.asarray([1 if x in [330.0,331.0] else 0 for x in labels])\n",
    "    \n",
    "#     #Balance data\n",
    "#     cancer = np.count_nonzero(labels)\n",
    "#     noncancer = (labels.shape[0]-cancer)\n",
    "#     minimum = min(cancer,noncancer)\n",
    "#     sample_idxs_cancer = random.sample(list(np.where(labels == 1)[0]), minimum)\n",
    "#     sample_idxs_nocancer = random.sample(list(np.where(labels == 0)[0]), minimum)\n",
    "    \n",
    "#     if mode=='dev':\n",
    "#         sample_idxs_cancer = sample_idxs_cancer[:1000]\n",
    "#         sample_idxs_nocancer = sample_idxs_nocancer[:1000] \n",
    "    \n",
    "#     new_idxs = []\n",
    "#     new_idxs.extend(sample_idxs_cancer)\n",
    "#     new_idxs.extend(sample_idxs_nocancer)\n",
    "#     random.shuffle(new_idxs)\n",
    "#     images = images[new_idxs]\n",
    "#     labels = labels[new_idxs]\n",
    "    \n",
    "#     pdb.set_trace()\n",
    "#     print(\"\\nLoaded\", len(images), \"patches \")\n",
    "    \n",
    "#     return images,labels\n",
    "\n",
    "# train_dataset_path = '/home/ubuntu/codebase/Semi-Supervised-GANs/dataset/patch_data/train'\n",
    "# dev_dataset_path = '/home/ubuntu/codebase/Semi-Supervised-GANs/dataset/patch_data/dev'\n",
    "\n",
    "# trainX,trainY = train_load(train_dataset_path,range(5,11))\n",
    "# devX,devY = train_load(dev_dataset_path,range(4,5),mode='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, Y, transform=None, mode='train'):\n",
    "        self.patches = X\n",
    "        self.labels = Y\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':                \n",
    "            return self.transform(Image.fromarray(self.patches[idx])),self.labels[idx]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            return self.transform(Image.fromarray(self.patches[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Defaults\n",
    "num_epochs = 10 \n",
    "batch_size = 32\n",
    "num_classes = 10 # number of classes for dataset\n",
    "lr = 0.0002 \n",
    "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam: decay of first order momentum of gradient\n",
    "n_cpu = 8 # number of cpu threads to use during batch generation\n",
    "latent_dim = 100 # dimensionality of the latent space\n",
    "img_size = 32 # size of each image dimension\n",
    "channels = 1 # number of output image channels\n",
    "sample_interval = 400 # interval between image sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cuda\n",
    "if torch.cuda.is_available():\n",
    "    cuda = True \n",
    "else:\n",
    "    cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.zero_()\n",
    "        \n",
    "        \n",
    "def initializer(m):\n",
    "    # Run xavier on all weights and zero all biases\n",
    "    if hasattr(m, 'weight'):\n",
    "        if m.weight.ndimension() > 1:\n",
    "            xavier_uniform_(m.weight.data)\n",
    "\n",
    "    if hasattr(m, 'bias') and m.bias is not None:\n",
    "        m.bias.data.zero_() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "          \n",
    "        dropout_rate = 0.5\n",
    "        filter1 = 96\n",
    "        filter2 = 192\n",
    "        \n",
    "        # Conv operations\n",
    "        # CNNBlock 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(filter1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=filter1, out_channels=filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(filter1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=filter1, out_channels=filter1, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(filter1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # CNNBlock 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(filter2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(filter2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(filter2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # CNNBlock 3\n",
    "        self.wn_conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(filter2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(filter2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(filter2),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "                \n",
    "        # Linear \n",
    "        self.linear = nn.Linear(in_features=filter2, out_features=num_classes)\n",
    "        self.apply(initializer)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional Operations\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        # Linear\n",
    "        x = x.mean(dim=3).mean(dim=2)\n",
    "        x = self.linear(flatten)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss and model\n",
    "loss = nn.BCELoss()\n",
    "model = Model()\n",
    "\n",
    "# Data Loader\n",
    "train_loader = get_loader(image_size, batch_size)\n",
    "\n",
    "# Initialize weights\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "if cuda:\n",
    "    loss.cuda()\n",
    "    model = nn.DataParallel(model)\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(optimizer, criterion, dataloader, model):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "        \n",
    "        imgs = Variable(imgs.float())\n",
    "        labels = Variable(labels.float())\n",
    "        #print(labels)\n",
    "        if cuda:\n",
    "            imgs = imgs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return np.mean(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Function\n",
    "def test(criterion, dataloader, model, mode = None):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "        \n",
    "        imgs = imgs.float()\n",
    "        labels = labels.float()\n",
    "        \n",
    "        if cuda:\n",
    "            imgs = imgs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if mode =='test':\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.long()).sum()\n",
    "                \n",
    "    epoch_loss = np.mean(epoch_loss)\n",
    "    \n",
    "    if mode == 'test':\n",
    "        test_accuracy = 100 * correct / total\n",
    "        return epoch_loss,test_accuracy\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/10], Train Loss = 725.466133, Validation Loss = 49.491781, Test Loss = 49.491781, Test Accuracy = 50.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-aa1d074545c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-71a53c321f79>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(optimizer, criterion, dataloader, model)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Main Function\n",
    "'''\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(optimizer, loss, train_loader, model)\n",
    "    valid_loss = test(loss, dev_loader, model)\n",
    "    test_loss, test_accuracy = test(loss, dev_loader, model,mode='test')\n",
    "    \n",
    "    # Display Progress\n",
    "    print (\"[Epoch %d/%d], Train Loss = %f, Validation Loss = %f, Test Loss = %f, Test Accuracy = %f\" % (epoch, num_epochs, train_loss, \n",
    "                                                                                    valid_loss, test_loss, test_accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
