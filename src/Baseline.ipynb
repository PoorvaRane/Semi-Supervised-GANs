{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.utils import weight_norm\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import pdb\n",
    "from logger import Logger\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "num_channels = 3\n",
    "num_classes = 1\n",
    "num_epochs = 300\n",
    "image_size = 32\n",
    "batch_size = 64\n",
    "epsilon = 1e-8 # used to avoid NAN loss\n",
    "logger = Logger('./logs')\n",
    "\n",
    "# Initialize parameters\n",
    "lr = 1e-5\n",
    "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam: decay of first order momentum of gradient\n",
    "\n",
    "model_path ='./baseline.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "class TCGADataset(Dataset):\n",
    "    def __init__(self, image_size, split):\n",
    "        self.split = split\n",
    "        self.tcga_dataset = self._create_dataset(image_size, split)\n",
    "        self.patches, self.labels = self.tcga_dataset\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "        ])\n",
    "        \n",
    "    def balance_data(self, images, labels):\n",
    "        cancer = np.count_nonzero(labels)\n",
    "        noncancer = (labels.shape[0] - cancer)\n",
    "        minimum = min(cancer, noncancer)\n",
    "        sample_idxs_cancer = random.sample(list(np.where(labels == 1)[0]), minimum)\n",
    "        sample_idxs_nocancer = random.sample(list(np.where(labels == 0)[0]), minimum)\n",
    "        new_idxs = []\n",
    "        new_idxs.extend(sample_idxs_cancer)\n",
    "        new_idxs.extend(sample_idxs_nocancer)\n",
    "        random.shuffle(new_idxs)\n",
    "        images = images[new_idxs]\n",
    "        labels = labels[new_idxs]\n",
    "        \n",
    "        # Print data statistics\n",
    "        print(\"Total number of patches : \",labels.shape[0])\n",
    "        print(\"Cancerous patches : \", sample_idxs_cancer.shape[0])\n",
    "        print(\"Non cancerous patches : \", sample_idxs_nocancer.shape[0])\n",
    "        \n",
    "        return images, labels\n",
    "    \n",
    "    def _create_dataset(self, image_size, split):\n",
    "        data_dir = '/mys3bucket/patch_data'\n",
    "        if self.split == 'train':\n",
    "            data_dir = os.path.join(data_dir, 'train')\n",
    "        else:\n",
    "            data_dir = os.path.join(data_dir, 'dev')\n",
    "        \n",
    "        all_files = os.listdir(data_dir)\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        # Iterate over all files\n",
    "        for file in all_files:\n",
    "            if '.npz' not in file:\n",
    "                continue\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            data = np.load(file_path)\n",
    "            X = data['arr_0']\n",
    "            y = data['arr_1']\n",
    "            images.append(X)\n",
    "            labels.append(y)                \n",
    "            \n",
    "        images = np.concatenate(images)\n",
    "        labels = np.concatenate(labels) \n",
    "        \n",
    "        #balance data\n",
    "        images, labels = self.balance_data(images, labels)\n",
    "            \n",
    "        return images, labels\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.patches[idx], self.labels[idx]\n",
    "        return self.transform(Image.fromarray(data)), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloaders\n",
    "def get_loader(image_size, batch_size):\n",
    "    num_workers = 2\n",
    "    tcga_train = TCGADataset(image_size=image_size, split='train')\n",
    "    tcga_dev = TCGADataset(image_size=image_size, split='dev')\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=tcga_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    dev_loader = DataLoader(\n",
    "        dataset=tcga_dev,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, dev_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.zero_()\n",
    "        \n",
    "        \n",
    "def initializer(m):\n",
    "    # Run xavier on all weights and zero all biases\n",
    "    if hasattr(m, 'weight'):\n",
    "        if m.weight.ndimension() > 1:\n",
    "            xavier_uniform_(m.weight.data)\n",
    "\n",
    "    if hasattr(m, 'bias') and m.bias is not None:\n",
    "        m.bias.data.zero_() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "          \n",
    "        dropout_rate = 0.2\n",
    "        filter1 = 96\n",
    "        filter2 = 192\n",
    "        \n",
    "        # Conv operations\n",
    "        # CNNBlock 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(filter1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=filter1, out_channels=filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(filter1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(filter2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # CNNBlock 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(filter2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(filter2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(filter2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # CNNBlock 3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(filter2),            \n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(filter2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(filter2),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "                \n",
    "        # Linear \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=filter2, out_features=(num_classes))\n",
    "        )\n",
    "        self.apply(initializer)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional Operations\n",
    "        x = self.conv1(x)\n",
    "        #x = self.conv2(x)\n",
    "        #x = self.conv3(x)\n",
    "        \n",
    "        # Linear\n",
    "        x = x.mean(dim=3).mean(dim=2)\n",
    "        x = self.linear(x)\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss and model\n",
    "criterion = nn.BCELoss()\n",
    "model = Model()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    criterion.cuda()\n",
    "    #model = nn.DataParallel(model)\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of patches :  102982\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-7d31e4473b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Data Loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialize weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-a71d5312c032>\u001b[0m in \u001b[0;36mget_loader\u001b[0;34m(image_size, batch_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtcga_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTCGADataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtcga_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTCGADataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-3b3a310555af>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_size, split)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtcga_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtcga_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         self.transform = transforms.Compose([\n",
      "\u001b[0;32m<ipython-input-36-3b3a310555af>\u001b[0m in \u001b[0;36m_create_dataset\u001b[0;34m(self, image_size, split)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m#balance data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbalance_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-3b3a310555af>\u001b[0m in \u001b[0;36mbalance_data\u001b[0;34m(self, images, labels)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Print data statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of patches : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cancerous patches : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_idxs_cancer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Non cancerous patches : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_idxs_nocancer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Data Loader\n",
    "train_loader, dev_loader = get_loader(image_size, batch_size)\n",
    "\n",
    "# Initialize weights\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(b1, b2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(epoch, num_epochs, optimizer, criterion, dataloader, model):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    loader_len = len(dataloader)\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        img, label = data\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "        b_size = img.size(0)\n",
    "    \n",
    "        # Loss computation\n",
    "        probs = model(img)\n",
    "        probs = probs.squeeze()\n",
    "        loss = criterion(probs, label.float())\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Train Accuracy Computation\n",
    "        compare = torch.FloatTensor([0.5])\n",
    "        if torch.cuda.is_available:\n",
    "            compare = compare.cuda()\n",
    "            \n",
    "        predicted = torch.ge(probs, compare)\n",
    "        correct = torch.sum(torch.eq(predicted.long(), label))\n",
    "        batch_accuracy = correct.item()/float(b_size)\n",
    "        total_acc += batch_accuracy\n",
    "        \n",
    "        # Print stats\n",
    "        if i%b_size == b_size-1:\n",
    "            print(\"Train [Epoch %d/%d] [Batch %d/%d] [loss: %f, acc: %d%%]\" % (epoch, num_epochs, i, \n",
    "                                       loader_len, loss.item(), 100 * batch_accuracy))\n",
    "            \n",
    "    total_loss = total_loss/float(i+1)\n",
    "    total_acc = total_acc/float(i+1)\n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Function\n",
    "def test(epoch, num_epochs, criterion, dataloader, model, mode):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    loader_len = len(dataloader)\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        \n",
    "        img, label = data\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "        b_size = img.size(0)\n",
    "    \n",
    "        # Loss computation\n",
    "        probs = model(img)\n",
    "        probs = probs.squeeze()\n",
    "        loss = criterion(probs, label.float())\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Train Accuracy Computation\n",
    "        compare = torch.FloatTensor([0.5])\n",
    "        if torch.cuda.is_available:\n",
    "            compare = compare.cuda()\n",
    "            \n",
    "        predicted = torch.ge(probs, compare)\n",
    "        correct = torch.sum(torch.eq(predicted.long(), label))\n",
    "        batch_accuracy = correct.item()/float(b_size)\n",
    "        total_acc += batch_accuracy\n",
    "        \n",
    "        # Print stats\n",
    "        if i%b_size == b_size-1:\n",
    "            print(\"%s [Epoch %d/%d] [Batch %d/%d] [loss: %f, acc: %d%%]\" % (mode, epoch, num_epochs, i, \n",
    "                                       loader_len, loss.item(), 100 * batch_accuracy))\n",
    "            \n",
    "    total_loss = total_loss/float(i+1)\n",
    "    total_acc = total_acc/float(i+1)\n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best):\n",
    "    if is_best:\n",
    "        torch.save(state, 'baseline_chkpt.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train [Epoch 0/300] [Batch 63/1541] [loss: 0.692138, acc: 51%]\n",
      "Train [Epoch 0/300] [Batch 127/1541] [loss: 0.700126, acc: 59%]\n",
      "Train [Epoch 0/300] [Batch 191/1541] [loss: 0.713397, acc: 46%]\n",
      "Train [Epoch 0/300] [Batch 255/1541] [loss: 0.690670, acc: 54%]\n",
      "Train [Epoch 0/300] [Batch 319/1541] [loss: 0.694571, acc: 56%]\n",
      "Train [Epoch 0/300] [Batch 383/1541] [loss: 0.710791, acc: 43%]\n",
      "Train [Epoch 0/300] [Batch 447/1541] [loss: 0.694960, acc: 48%]\n",
      "Train [Epoch 0/300] [Batch 511/1541] [loss: 0.694278, acc: 50%]\n",
      "Train [Epoch 0/300] [Batch 575/1541] [loss: 0.715602, acc: 40%]\n",
      "Train [Epoch 0/300] [Batch 639/1541] [loss: 0.706441, acc: 43%]\n",
      "Train [Epoch 0/300] [Batch 703/1541] [loss: 0.702206, acc: 46%]\n",
      "Train [Epoch 0/300] [Batch 767/1541] [loss: 0.688915, acc: 48%]\n",
      "Train [Epoch 0/300] [Batch 831/1541] [loss: 0.682634, acc: 54%]\n",
      "Train [Epoch 0/300] [Batch 895/1541] [loss: 0.736560, acc: 35%]\n",
      "Train [Epoch 0/300] [Batch 959/1541] [loss: 0.703839, acc: 42%]\n",
      "Train [Epoch 0/300] [Batch 1023/1541] [loss: 0.732782, acc: 50%]\n",
      "Train [Epoch 0/300] [Batch 1087/1541] [loss: 0.727373, acc: 48%]\n",
      "Train [Epoch 0/300] [Batch 1151/1541] [loss: 0.710913, acc: 50%]\n",
      "Train [Epoch 0/300] [Batch 1215/1541] [loss: 0.708581, acc: 45%]\n",
      "Train [Epoch 0/300] [Batch 1279/1541] [loss: 0.731245, acc: 39%]\n",
      "Train [Epoch 0/300] [Batch 1343/1541] [loss: 0.709243, acc: 45%]\n",
      "Train [Epoch 0/300] [Batch 1407/1541] [loss: 0.707928, acc: 48%]\n",
      "Train [Epoch 0/300] [Batch 1471/1541] [loss: 0.710050, acc: 46%]\n",
      "Train [Epoch 0/300] [Batch 1535/1541] [loss: 0.727418, acc: 39%]\n",
      "------\n",
      "Valid [Epoch 0/300] [Batch 63/635] [loss: 0.675178, acc: 56%]\n",
      "Valid [Epoch 0/300] [Batch 127/635] [loss: 0.662751, acc: 67%]\n",
      "Valid [Epoch 0/300] [Batch 191/635] [loss: 0.679786, acc: 51%]\n",
      "Valid [Epoch 0/300] [Batch 255/635] [loss: 0.677718, acc: 51%]\n",
      "Valid [Epoch 0/300] [Batch 319/635] [loss: 0.687604, acc: 48%]\n",
      "Valid [Epoch 0/300] [Batch 383/635] [loss: 0.678164, acc: 51%]\n",
      "Valid [Epoch 0/300] [Batch 447/635] [loss: 0.687789, acc: 54%]\n",
      "Valid [Epoch 0/300] [Batch 511/635] [loss: 0.658264, acc: 65%]\n",
      "Valid [Epoch 0/300] [Batch 575/635] [loss: 0.658926, acc: 64%]\n",
      "--------------------------------------------------------------------\n",
      "Train ===> [Epoch 0/300] [Train loss: 0.708051, train acc: 47%]\n",
      "Valid ===> [Epoch 0/300] [Valid loss: 0.675507, valid acc: 56%]\n",
      "--------------------------------------------------------------------\n",
      "Train [Epoch 1/300] [Batch 63/1541] [loss: 0.683783, acc: 51%]\n",
      "Train [Epoch 1/300] [Batch 127/1541] [loss: 0.735473, acc: 42%]\n",
      "Train [Epoch 1/300] [Batch 191/1541] [loss: 0.699370, acc: 53%]\n",
      "Train [Epoch 1/300] [Batch 255/1541] [loss: 0.694934, acc: 59%]\n",
      "Train [Epoch 1/300] [Batch 319/1541] [loss: 0.697551, acc: 51%]\n",
      "Train [Epoch 1/300] [Batch 383/1541] [loss: 0.713885, acc: 43%]\n",
      "Train [Epoch 1/300] [Batch 447/1541] [loss: 0.679906, acc: 54%]\n",
      "Train [Epoch 1/300] [Batch 511/1541] [loss: 0.710077, acc: 48%]\n",
      "Train [Epoch 1/300] [Batch 575/1541] [loss: 0.663374, acc: 57%]\n",
      "Train [Epoch 1/300] [Batch 639/1541] [loss: 0.685634, acc: 53%]\n",
      "Train [Epoch 1/300] [Batch 703/1541] [loss: 0.737052, acc: 37%]\n",
      "Train [Epoch 1/300] [Batch 767/1541] [loss: 0.718169, acc: 42%]\n",
      "Train [Epoch 1/300] [Batch 831/1541] [loss: 0.694231, acc: 53%]\n",
      "Train [Epoch 1/300] [Batch 895/1541] [loss: 0.701193, acc: 50%]\n",
      "Train [Epoch 1/300] [Batch 959/1541] [loss: 0.735767, acc: 35%]\n",
      "Train [Epoch 1/300] [Batch 1023/1541] [loss: 0.727245, acc: 43%]\n",
      "Train [Epoch 1/300] [Batch 1087/1541] [loss: 0.686126, acc: 56%]\n",
      "Train [Epoch 1/300] [Batch 1151/1541] [loss: 0.702746, acc: 50%]\n",
      "Train [Epoch 1/300] [Batch 1215/1541] [loss: 0.705882, acc: 43%]\n",
      "Train [Epoch 1/300] [Batch 1279/1541] [loss: 0.695048, acc: 45%]\n",
      "Train [Epoch 1/300] [Batch 1343/1541] [loss: 0.695951, acc: 56%]\n",
      "Train [Epoch 1/300] [Batch 1407/1541] [loss: 0.721496, acc: 42%]\n",
      "Train [Epoch 1/300] [Batch 1471/1541] [loss: 0.720703, acc: 48%]\n",
      "Train [Epoch 1/300] [Batch 1535/1541] [loss: 0.719557, acc: 42%]\n",
      "------\n",
      "Valid [Epoch 1/300] [Batch 63/635] [loss: 0.667880, acc: 60%]\n",
      "Valid [Epoch 1/300] [Batch 127/635] [loss: 0.672086, acc: 59%]\n",
      "Valid [Epoch 1/300] [Batch 191/635] [loss: 0.654348, acc: 67%]\n",
      "Valid [Epoch 1/300] [Batch 255/635] [loss: 0.681757, acc: 56%]\n",
      "Valid [Epoch 1/300] [Batch 319/635] [loss: 0.673609, acc: 54%]\n",
      "Valid [Epoch 1/300] [Batch 383/635] [loss: 0.660574, acc: 64%]\n",
      "Valid [Epoch 1/300] [Batch 447/635] [loss: 0.666651, acc: 59%]\n",
      "Valid [Epoch 1/300] [Batch 511/635] [loss: 0.664322, acc: 64%]\n",
      "Valid [Epoch 1/300] [Batch 575/635] [loss: 0.680687, acc: 53%]\n",
      "--------------------------------------------------------------------\n",
      "Train ===> [Epoch 1/300] [Train loss: 0.708530, train acc: 47%]\n",
      "Valid ===> [Epoch 1/300] [Valid loss: 0.675859, valid acc: 56%]\n",
      "--------------------------------------------------------------------\n",
      "Train [Epoch 2/300] [Batch 63/1541] [loss: 0.719325, acc: 50%]\n",
      "Train [Epoch 2/300] [Batch 127/1541] [loss: 0.716572, acc: 48%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-45:\n",
      "Process Process-46:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-20e2a09f7373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-5f5ebd2e3c55>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, num_epochs, optimizer, criterion, dataloader, model)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mcompare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mcompare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Call Train and Test and save best model\n",
    "'''\n",
    "best_valid_acc = 0.0\n",
    "best_valid_loss = 99999.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(epoch, num_epochs, optimizer, criterion, train_loader, model)\n",
    "    print('------')\n",
    "    valid_loss, valid_acc = test(epoch, num_epochs, criterion, dev_loader, model, 'Valid')\n",
    "    \n",
    "    print('--------------------------------------------------------------------')\n",
    "    print(\"Train ===> [Epoch %d/%d] [Train loss: %f, train acc: %d%%]\" % (epoch, num_epochs, \n",
    "                                                                          train_loss, 100 * train_acc))\n",
    "    print(\"Valid ===> [Epoch %d/%d] [Valid loss: %f, valid acc: %d%%]\" % (epoch, num_epochs, \n",
    "                                                                          valid_loss, 100 * valid_acc))\n",
    "    print('--------------------------------------------------------------------')\n",
    "    \n",
    "    # Save best model\n",
    "    is_best = valid_acc >= best_valid_acc\n",
    "    save_checkpoint({\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer' : optimizer.state_dict(),\n",
    "    'best_acc' : valid_acc\n",
    "    }, is_best)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
