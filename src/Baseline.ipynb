{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Set Defaults\n",
    "'''\n",
    "num_epochs = 200 \n",
    "batch_size = 64\n",
    "num_classes = 10 # number of classes for dataset\n",
    "lr = 0.0002 \n",
    "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam: decay of first order momentum of gradient\n",
    "n_cpu = 8 # number of cpu threads to use during batch generation\n",
    "latent_dim = 100 # dimensionality of the latent space\n",
    "img_size = 32 # size of each image dimension\n",
    "channels = 1 # number of output image channels\n",
    "sample_interval = 400 # interval between image sampling\n",
    "ndf = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set cuda\n",
    "if torch.cuda.is_available():\n",
    "    cuda = True \n",
    "else:\n",
    "    cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, ndf, alpha, nc, drop_rate, num_classes):\n",
    "        super(net, self).__init__()\n",
    "        self.use_gpu = cuda\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=nc, out_channels=ndf, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(alpha),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=ndf, out_channels=ndf*2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*2),\n",
    "            nn.LeakyReLU(alpha),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=ndf*2, out_channels=ndf*4, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*4),\n",
    "            nn.LeakyReLU(alpha),\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=ndf*4, out_channels=ndf*8, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*8),\n",
    "            nn.LeakyReLU(alpha),\n",
    "        )\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            # input is (number_channels) x 32 x 32\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            nn.Dropout2d(drop_rate),\n",
    "            # (ndf) x 16 x 16\n",
    "            nn.Conv2d(ndf, ndf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ndf) x 8 x 8\n",
    "            nn.Conv2d(ndf, ndf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            nn.Dropout2d(drop_rate),\n",
    "            # (ndf) x 4 x 4\n",
    "            nn.Conv2d(ndf, ndf * 2, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ndf * 2) x 4 x 4\n",
    "            nn.Conv2d(ndf * 2, ndf * 2, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ndf * 2) x 4 x 4\n",
    "            nn.Conv2d(ndf * 2, ndf * 2, 3, 1, 0, bias=False),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ndf * 2) x 2 x 2\n",
    "        )\n",
    "        \n",
    "        self.features = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "        self.class_logits = nn.Linear(\n",
    "            in_features=(ndf * 2) * 1 * 1,\n",
    "            out_features=num_classes + 1)\n",
    "        \n",
    "        self.gan_logits = _ganLogits(num_classes)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "#         if isinstance(inputs.data, torch.cuda.FloatTensor) and self.use_gpu:\n",
    "#             out = nn.parallel.data_parallel(self.main, inputs, range(1))\n",
    "#         else:\n",
    "        out = self.main(inputs)\n",
    "\n",
    "        features = self.features(out)\n",
    "        print(\"features before = \", features.size())\n",
    "        features = features.squeeze()\n",
    "        print(\"features after = \", features.size())\n",
    "\n",
    "        class_logits = self.class_logits(features)\n",
    "        print(\"class logits = \", class_logits.size())\n",
    "\n",
    "        gan_logits = self.gan_logits(class_logits)\n",
    "        print(\"gan_logits = \", gan_logits.size())\n",
    "        \n",
    "        out = self.softmax(class_logits)\n",
    "\n",
    "        return out, class_logits, gan_logits, features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
