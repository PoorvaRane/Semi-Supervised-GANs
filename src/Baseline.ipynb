{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.utils import weight_norm\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import pdb\n",
    "from logger import Logger\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "num_channels = 3\n",
    "num_classes = 1\n",
    "num_epochs = 300\n",
    "image_size = 32\n",
    "batch_size = 64\n",
    "epsilon = 1e-8 # used to avoid NAN loss\n",
    "logger = Logger('./logs')\n",
    "\n",
    "# Initialize parameters\n",
    "lr = 1e-5\n",
    "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam: decay of first order momentum of gradient\n",
    "\n",
    "model_path ='./baseline.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "class TCGADataset(Dataset):\n",
    "    def __init__(self, image_size, split):\n",
    "        self.split = split\n",
    "        self.tcga_dataset = self._create_dataset(image_size, split)\n",
    "        self.patches, self.labels = self.tcga_dataset\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "        ])\n",
    "        \n",
    "    def balance_data(self, images, labels):\n",
    "        cancer = np.count_nonzero(labels)\n",
    "        noncancer = (labels.shape[0] - cancer)\n",
    "        minimum = min(cancer, noncancer)\n",
    "        sample_idxs_cancer = random.sample(list(np.where(labels == 1)[0]), minimum)\n",
    "        sample_idxs_nocancer = random.sample(list(np.where(labels == 0)[0]), minimum)\n",
    "        new_idxs = []\n",
    "        new_idxs.extend(sample_idxs_cancer)\n",
    "        new_idxs.extend(sample_idxs_nocancer)\n",
    "        random.shuffle(new_idxs)\n",
    "        images = images[new_idxs]\n",
    "        labels = labels[new_idxs]\n",
    "        \n",
    "        # Print data statistics\n",
    "        print(\"Total number of patches : \",labels.shape[0])\n",
    "        print(\"Cancerous patches : \", len(sample_idxs_cancer))\n",
    "        print(\"Non cancerous patches : \", len(sample_idxs_nocancer))\n",
    "        \n",
    "        return images, labels\n",
    "    \n",
    "    def _create_dataset(self, image_size, split):\n",
    "        data_dir = '/mys3bucket/patch_data'\n",
    "        if self.split == 'train':\n",
    "            data_dir = os.path.join(data_dir, 'train')\n",
    "        else:\n",
    "            data_dir = os.path.join(data_dir, 'dev')\n",
    "        \n",
    "        all_files = os.listdir(data_dir)\n",
    "        images = []\n",
    "        labels = []\n",
    "        \n",
    "        # Iterate over all files\n",
    "        for file in all_files:\n",
    "            if '.npz' not in file:\n",
    "                continue\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            data = np.load(file_path)\n",
    "            X = data['arr_0']\n",
    "            y = data['arr_1']\n",
    "            images.append(X)\n",
    "            labels.append(y)                \n",
    "            \n",
    "        images = np.concatenate(images)\n",
    "        labels = np.concatenate(labels) \n",
    "        \n",
    "        #balance data\n",
    "        images, labels = self.balance_data(images, labels)\n",
    "            \n",
    "        return images, labels\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.patches[idx], self.labels[idx]\n",
    "        return self.transform(Image.fromarray(data)), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloaders\n",
    "def get_loader(image_size, batch_size):\n",
    "    num_workers = 2\n",
    "    tcga_train = TCGADataset(image_size=image_size, split='train')\n",
    "    tcga_dev = TCGADataset(image_size=image_size, split='dev')\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=tcga_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    dev_loader = DataLoader(\n",
    "        dataset=tcga_dev,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, dev_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.zero_()\n",
    "        \n",
    "        \n",
    "def initializer(m):\n",
    "    # Run xavier on all weights and zero all biases\n",
    "    if hasattr(m, 'weight'):\n",
    "        if m.weight.ndimension() > 1:\n",
    "            xavier_uniform_(m.weight.data)\n",
    "\n",
    "    if hasattr(m, 'bias') and m.bias is not None:\n",
    "        m.bias.data.zero_() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "          \n",
    "        dropout_rate = 0.2\n",
    "        filter1 = 96\n",
    "        filter2 = 192\n",
    "        \n",
    "        # Conv operations\n",
    "        # CNNBlock 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=filter1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(filter1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "#             nn.Conv2d(in_channels=filter1, out_channels=filter1, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(filter1),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=3, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(filter1),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # CNNBlock 2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=filter1, out_channels=filter2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(filter2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "#             nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(filter2),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=3, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(filter2),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # CNNBlock 3\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(filter2),            \n",
    "            nn.LeakyReLU(0.2),\n",
    "#             nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(filter2),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Conv2d(in_channels=filter2, out_channels=filter2, kernel_size=1, stride=1, padding=0),\n",
    "#             nn.BatchNorm2d(filter2),\n",
    "#             nn.LeakyReLU(0.2)\n",
    "        )\n",
    "                \n",
    "        # Linear \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=filter2, out_features=(num_classes))\n",
    "        )\n",
    "        self.apply(initializer)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional Operations\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        # Linear\n",
    "        x = x.mean(dim=3).mean(dim=2)\n",
    "        x = self.linear(x)\n",
    "        x = F.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss and model\n",
    "criterion = nn.BCELoss()\n",
    "model = Model()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    criterion.cuda()\n",
    "    #model = nn.DataParallel(model)\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of patches :  102982\n",
      "Cancerous patches :  51491\n",
      "Non cancerous patches :  51491\n",
      "Total number of patches :  40600\n",
      "Cancerous patches :  20300\n",
      "Non cancerous patches :  20300\n"
     ]
    }
   ],
   "source": [
    "# Data Loader\n",
    "train_loader, dev_loader = get_loader(image_size, batch_size)\n",
    "\n",
    "# Initialize weights\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(b1, b2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(epoch, num_epochs, optimizer, criterion, dataloader, model):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    loader_len = len(dataloader)\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        img, label = data\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "        b_size = img.size(0)\n",
    "    \n",
    "        # Loss computation\n",
    "        probs = model(img)\n",
    "        probs = probs.squeeze()\n",
    "        loss = criterion(probs, label.float())\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Train Accuracy Computation\n",
    "        compare = torch.FloatTensor([0.5])\n",
    "        if torch.cuda.is_available:\n",
    "            compare = compare.cuda()\n",
    "            \n",
    "        predicted = torch.ge(probs, compare)\n",
    "        correct = torch.sum(torch.eq(predicted.long(), label))\n",
    "        batch_accuracy = correct.item()/float(b_size)\n",
    "        total_acc += batch_accuracy\n",
    "        \n",
    "        # Print stats\n",
    "        if i%b_size == b_size-1:\n",
    "            print(\"Train [Epoch %d/%d] [Batch %d/%d] [loss: %f, acc: %d%%]\" % (epoch, num_epochs, i, \n",
    "                                       loader_len, loss.item(), 100 * batch_accuracy))\n",
    "            \n",
    "    total_loss = total_loss/float(i+1)\n",
    "    total_acc = total_acc/float(i+1)\n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Function\n",
    "def test(epoch, num_epochs, criterion, dataloader, model, mode):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    loader_len = len(dataloader)\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "        \n",
    "        img, label = data\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "        b_size = img.size(0)\n",
    "    \n",
    "        # Loss computation\n",
    "        probs = model(img)\n",
    "        probs = probs.squeeze()\n",
    "        loss = criterion(probs, label.float())\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Train Accuracy Computation\n",
    "        compare = torch.FloatTensor([0.5])\n",
    "        if torch.cuda.is_available:\n",
    "            compare = compare.cuda()\n",
    "            \n",
    "        predicted = torch.ge(probs, compare)\n",
    "        correct = torch.sum(torch.eq(predicted.long(), label))\n",
    "        batch_accuracy = correct.item()/float(b_size)\n",
    "        total_acc += batch_accuracy\n",
    "        \n",
    "        # Print stats\n",
    "        if i%b_size == b_size-1:\n",
    "            print(\"%s [Epoch %d/%d] [Batch %d/%d] [loss: %f, acc: %d%%]\" % (mode, epoch, num_epochs, i, \n",
    "                                       loader_len, loss.item(), 100 * batch_accuracy))\n",
    "            \n",
    "    total_loss = total_loss/float(i+1)\n",
    "    total_acc = total_acc/float(i+1)\n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best):\n",
    "    if is_best:\n",
    "        torch.save(state, 'baseline_chkpt.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(epoch, train, dev, mode):\n",
    "    epoch_list = np.arange(epoch + 1)\n",
    "    plt.plot(epoch_list, train)\n",
    "    plt.plot(epoch_list, dev)\n",
    "    \n",
    "    if mode.lower() == 'accuracy':\n",
    "        location = 'lower right'\n",
    "    else:\n",
    "        location = 'upper right'\n",
    "\n",
    "    plt.legend(['Train ' +  mode, 'Dev ' + mode], loc=location)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt_image_path = os.path.join(graph_dir, 'Baseline_' + mode.lower()[:4] + '_epoch_' + str(epoch))\n",
    "    plt.savefig(plt_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train [Epoch 0/300] [Batch 63/1610] [loss: 0.633358, acc: 65%]\n",
      "Train [Epoch 0/300] [Batch 127/1610] [loss: 0.664521, acc: 59%]\n",
      "Train [Epoch 0/300] [Batch 191/1610] [loss: 0.555386, acc: 70%]\n",
      "Train [Epoch 0/300] [Batch 255/1610] [loss: 0.560404, acc: 70%]\n",
      "Train [Epoch 0/300] [Batch 319/1610] [loss: 0.590328, acc: 65%]\n",
      "Train [Epoch 0/300] [Batch 383/1610] [loss: 0.648236, acc: 65%]\n",
      "Train [Epoch 0/300] [Batch 447/1610] [loss: 0.616307, acc: 64%]\n",
      "Train [Epoch 0/300] [Batch 511/1610] [loss: 0.676630, acc: 62%]\n",
      "Train [Epoch 0/300] [Batch 575/1610] [loss: 0.497560, acc: 76%]\n",
      "Train [Epoch 0/300] [Batch 639/1610] [loss: 0.585045, acc: 70%]\n",
      "Train [Epoch 0/300] [Batch 703/1610] [loss: 0.552985, acc: 67%]\n",
      "Train [Epoch 0/300] [Batch 767/1610] [loss: 0.581476, acc: 71%]\n",
      "Train [Epoch 0/300] [Batch 831/1610] [loss: 0.578952, acc: 67%]\n",
      "Train [Epoch 0/300] [Batch 895/1610] [loss: 0.574870, acc: 62%]\n",
      "Train [Epoch 0/300] [Batch 959/1610] [loss: 0.554427, acc: 71%]\n",
      "Train [Epoch 0/300] [Batch 1023/1610] [loss: 0.548620, acc: 70%]\n",
      "Train [Epoch 0/300] [Batch 1087/1610] [loss: 0.606960, acc: 65%]\n",
      "Train [Epoch 0/300] [Batch 1151/1610] [loss: 0.598370, acc: 62%]\n",
      "Train [Epoch 0/300] [Batch 1215/1610] [loss: 0.556340, acc: 76%]\n",
      "Train [Epoch 0/300] [Batch 1279/1610] [loss: 0.547015, acc: 75%]\n",
      "Train [Epoch 0/300] [Batch 1343/1610] [loss: 0.481261, acc: 76%]\n",
      "Train [Epoch 0/300] [Batch 1407/1610] [loss: 0.582134, acc: 73%]\n",
      "Train [Epoch 0/300] [Batch 1471/1610] [loss: 0.529004, acc: 71%]\n",
      "Train [Epoch 0/300] [Batch 1535/1610] [loss: 0.569353, acc: 71%]\n",
      "Train [Epoch 0/300] [Batch 1599/1610] [loss: 0.465994, acc: 82%]\n",
      "------\n",
      "Valid [Epoch 0/300] [Batch 63/635] [loss: 0.498770, acc: 78%]\n",
      "Valid [Epoch 0/300] [Batch 127/635] [loss: 0.459139, acc: 81%]\n",
      "Valid [Epoch 0/300] [Batch 191/635] [loss: 0.515281, acc: 76%]\n",
      "Valid [Epoch 0/300] [Batch 255/635] [loss: 0.472643, acc: 78%]\n",
      "Valid [Epoch 0/300] [Batch 319/635] [loss: 0.560970, acc: 68%]\n",
      "Valid [Epoch 0/300] [Batch 383/635] [loss: 0.549769, acc: 71%]\n",
      "Valid [Epoch 0/300] [Batch 447/635] [loss: 0.531902, acc: 73%]\n",
      "Valid [Epoch 0/300] [Batch 511/635] [loss: 0.540265, acc: 67%]\n",
      "Valid [Epoch 0/300] [Batch 575/635] [loss: 0.566632, acc: 73%]\n",
      "--------------------------------------------------------------------\n",
      "Train ===> [Epoch 0/300] [Train loss: 0.585326, train acc: 68%]\n",
      "Valid ===> [Epoch 0/300] [Valid loss: 0.534673, valid acc: 72%]\n",
      "--------------------------------------------------------------------\n",
      "Train [Epoch 1/300] [Batch 63/1610] [loss: 0.613216, acc: 62%]\n",
      "Train [Epoch 1/300] [Batch 127/1610] [loss: 0.586308, acc: 73%]\n",
      "Train [Epoch 1/300] [Batch 191/1610] [loss: 0.574152, acc: 68%]\n",
      "Train [Epoch 1/300] [Batch 255/1610] [loss: 0.524804, acc: 67%]\n",
      "Train [Epoch 1/300] [Batch 319/1610] [loss: 0.494012, acc: 71%]\n",
      "Train [Epoch 1/300] [Batch 383/1610] [loss: 0.620360, acc: 65%]\n",
      "Train [Epoch 1/300] [Batch 447/1610] [loss: 0.596695, acc: 67%]\n",
      "Train [Epoch 1/300] [Batch 511/1610] [loss: 0.545060, acc: 70%]\n",
      "Train [Epoch 1/300] [Batch 575/1610] [loss: 0.569043, acc: 71%]\n",
      "Train [Epoch 1/300] [Batch 639/1610] [loss: 0.531214, acc: 73%]\n",
      "Train [Epoch 1/300] [Batch 703/1610] [loss: 0.482610, acc: 79%]\n",
      "Train [Epoch 1/300] [Batch 767/1610] [loss: 0.517113, acc: 76%]\n",
      "Train [Epoch 1/300] [Batch 831/1610] [loss: 0.497510, acc: 76%]\n",
      "Train [Epoch 1/300] [Batch 895/1610] [loss: 0.532918, acc: 76%]\n",
      "Train [Epoch 1/300] [Batch 959/1610] [loss: 0.513299, acc: 78%]\n",
      "Train [Epoch 1/300] [Batch 1023/1610] [loss: 0.519212, acc: 70%]\n",
      "Train [Epoch 1/300] [Batch 1087/1610] [loss: 0.569149, acc: 67%]\n",
      "Train [Epoch 1/300] [Batch 1151/1610] [loss: 0.502115, acc: 76%]\n",
      "Train [Epoch 1/300] [Batch 1215/1610] [loss: 0.575848, acc: 67%]\n",
      "Train [Epoch 1/300] [Batch 1279/1610] [loss: 0.493782, acc: 78%]\n",
      "Train [Epoch 1/300] [Batch 1343/1610] [loss: 0.634674, acc: 64%]\n",
      "Train [Epoch 1/300] [Batch 1407/1610] [loss: 0.574310, acc: 67%]\n",
      "Train [Epoch 1/300] [Batch 1471/1610] [loss: 0.543973, acc: 68%]\n",
      "Train [Epoch 1/300] [Batch 1535/1610] [loss: 0.473449, acc: 73%]\n",
      "Train [Epoch 1/300] [Batch 1599/1610] [loss: 0.509291, acc: 79%]\n",
      "------\n",
      "Valid [Epoch 1/300] [Batch 63/635] [loss: 0.501101, acc: 79%]\n",
      "Valid [Epoch 1/300] [Batch 127/635] [loss: 0.478465, acc: 78%]\n",
      "Valid [Epoch 1/300] [Batch 191/635] [loss: 0.562128, acc: 71%]\n",
      "Valid [Epoch 1/300] [Batch 255/635] [loss: 0.584944, acc: 68%]\n",
      "Valid [Epoch 1/300] [Batch 319/635] [loss: 0.601889, acc: 67%]\n",
      "Valid [Epoch 1/300] [Batch 383/635] [loss: 0.472182, acc: 76%]\n",
      "Valid [Epoch 1/300] [Batch 447/635] [loss: 0.552120, acc: 65%]\n",
      "Valid [Epoch 1/300] [Batch 511/635] [loss: 0.432619, acc: 81%]\n",
      "Valid [Epoch 1/300] [Batch 575/635] [loss: 0.514046, acc: 73%]\n",
      "--------------------------------------------------------------------\n",
      "Train ===> [Epoch 1/300] [Train loss: 0.547741, train acc: 72%]\n",
      "Valid ===> [Epoch 1/300] [Valid loss: 0.532671, valid acc: 72%]\n",
      "--------------------------------------------------------------------\n",
      "Train [Epoch 2/300] [Batch 63/1610] [loss: 0.484849, acc: 73%]\n",
      "Train [Epoch 2/300] [Batch 127/1610] [loss: 0.564134, acc: 71%]\n",
      "Train [Epoch 2/300] [Batch 191/1610] [loss: 0.431021, acc: 79%]\n",
      "Train [Epoch 2/300] [Batch 255/1610] [loss: 0.494669, acc: 75%]\n",
      "Train [Epoch 2/300] [Batch 319/1610] [loss: 0.496441, acc: 79%]\n",
      "Train [Epoch 2/300] [Batch 383/1610] [loss: 0.438815, acc: 81%]\n",
      "Train [Epoch 2/300] [Batch 447/1610] [loss: 0.499046, acc: 75%]\n",
      "Train [Epoch 2/300] [Batch 511/1610] [loss: 0.561920, acc: 65%]\n",
      "Train [Epoch 2/300] [Batch 575/1610] [loss: 0.498548, acc: 73%]\n",
      "Train [Epoch 2/300] [Batch 639/1610] [loss: 0.481597, acc: 81%]\n",
      "Train [Epoch 2/300] [Batch 703/1610] [loss: 0.522464, acc: 73%]\n",
      "Train [Epoch 2/300] [Batch 767/1610] [loss: 0.497580, acc: 79%]\n",
      "Train [Epoch 2/300] [Batch 831/1610] [loss: 0.524801, acc: 71%]\n",
      "Train [Epoch 2/300] [Batch 895/1610] [loss: 0.622649, acc: 60%]\n",
      "Train [Epoch 2/300] [Batch 959/1610] [loss: 0.571438, acc: 75%]\n",
      "Train [Epoch 2/300] [Batch 1023/1610] [loss: 0.502440, acc: 73%]\n",
      "Train [Epoch 2/300] [Batch 1087/1610] [loss: 0.543007, acc: 75%]\n",
      "Train [Epoch 2/300] [Batch 1151/1610] [loss: 0.508033, acc: 75%]\n",
      "Train [Epoch 2/300] [Batch 1215/1610] [loss: 0.457008, acc: 79%]\n",
      "Train [Epoch 2/300] [Batch 1279/1610] [loss: 0.518207, acc: 71%]\n",
      "Train [Epoch 2/300] [Batch 1343/1610] [loss: 0.509885, acc: 71%]\n",
      "Train [Epoch 2/300] [Batch 1407/1610] [loss: 0.486604, acc: 75%]\n",
      "Train [Epoch 2/300] [Batch 1471/1610] [loss: 0.519501, acc: 76%]\n",
      "Train [Epoch 2/300] [Batch 1535/1610] [loss: 0.484585, acc: 75%]\n",
      "Train [Epoch 2/300] [Batch 1599/1610] [loss: 0.467461, acc: 78%]\n",
      "------\n",
      "Valid [Epoch 2/300] [Batch 63/635] [loss: 0.539143, acc: 65%]\n",
      "Valid [Epoch 2/300] [Batch 127/635] [loss: 0.493083, acc: 81%]\n",
      "Valid [Epoch 2/300] [Batch 191/635] [loss: 0.536422, acc: 71%]\n",
      "Valid [Epoch 2/300] [Batch 255/635] [loss: 0.509139, acc: 73%]\n",
      "Valid [Epoch 2/300] [Batch 319/635] [loss: 0.421510, acc: 89%]\n",
      "Valid [Epoch 2/300] [Batch 383/635] [loss: 0.592979, acc: 71%]\n",
      "Valid [Epoch 2/300] [Batch 447/635] [loss: 0.544525, acc: 67%]\n",
      "Valid [Epoch 2/300] [Batch 511/635] [loss: 0.534335, acc: 70%]\n",
      "Valid [Epoch 2/300] [Batch 575/635] [loss: 0.537772, acc: 68%]\n",
      "--------------------------------------------------------------------\n",
      "Train ===> [Epoch 2/300] [Train loss: 0.531163, train acc: 74%]\n",
      "Valid ===> [Epoch 2/300] [Valid loss: 0.544251, valid acc: 72%]\n",
      "--------------------------------------------------------------------\n",
      "Train [Epoch 3/300] [Batch 63/1610] [loss: 0.501287, acc: 71%]\n",
      "Train [Epoch 3/300] [Batch 127/1610] [loss: 0.500339, acc: 79%]\n",
      "Train [Epoch 3/300] [Batch 191/1610] [loss: 0.513199, acc: 75%]\n",
      "Train [Epoch 3/300] [Batch 255/1610] [loss: 0.406789, acc: 82%]\n",
      "Train [Epoch 3/300] [Batch 319/1610] [loss: 0.466393, acc: 78%]\n",
      "Train [Epoch 3/300] [Batch 383/1610] [loss: 0.503404, acc: 78%]\n",
      "Train [Epoch 3/300] [Batch 447/1610] [loss: 0.535931, acc: 75%]\n",
      "Train [Epoch 3/300] [Batch 511/1610] [loss: 0.589777, acc: 68%]\n",
      "Train [Epoch 3/300] [Batch 575/1610] [loss: 0.466902, acc: 78%]\n",
      "Train [Epoch 3/300] [Batch 639/1610] [loss: 0.540161, acc: 73%]\n",
      "Train [Epoch 3/300] [Batch 703/1610] [loss: 0.462753, acc: 73%]\n",
      "Train [Epoch 3/300] [Batch 767/1610] [loss: 0.516171, acc: 73%]\n",
      "Train [Epoch 3/300] [Batch 831/1610] [loss: 0.429272, acc: 82%]\n",
      "Train [Epoch 3/300] [Batch 895/1610] [loss: 0.459516, acc: 81%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train [Epoch 3/300] [Batch 959/1610] [loss: 0.570591, acc: 76%]\n",
      "Train [Epoch 3/300] [Batch 1023/1610] [loss: 0.507868, acc: 75%]\n",
      "Train [Epoch 3/300] [Batch 1087/1610] [loss: 0.575538, acc: 73%]\n",
      "Train [Epoch 3/300] [Batch 1151/1610] [loss: 0.545072, acc: 67%]\n",
      "Train [Epoch 3/300] [Batch 1215/1610] [loss: 0.477180, acc: 79%]\n",
      "Train [Epoch 3/300] [Batch 1279/1610] [loss: 0.511000, acc: 81%]\n",
      "Train [Epoch 3/300] [Batch 1343/1610] [loss: 0.498666, acc: 76%]\n",
      "Train [Epoch 3/300] [Batch 1407/1610] [loss: 0.485995, acc: 78%]\n",
      "Train [Epoch 3/300] [Batch 1471/1610] [loss: 0.487385, acc: 79%]\n",
      "Train [Epoch 3/300] [Batch 1535/1610] [loss: 0.619923, acc: 64%]\n",
      "Train [Epoch 3/300] [Batch 1599/1610] [loss: 0.523130, acc: 79%]\n",
      "------\n",
      "Valid [Epoch 3/300] [Batch 63/635] [loss: 0.571448, acc: 73%]\n",
      "Valid [Epoch 3/300] [Batch 127/635] [loss: 0.561034, acc: 64%]\n",
      "Valid [Epoch 3/300] [Batch 191/635] [loss: 0.622542, acc: 70%]\n",
      "Valid [Epoch 3/300] [Batch 255/635] [loss: 0.514652, acc: 75%]\n",
      "Valid [Epoch 3/300] [Batch 319/635] [loss: 0.529522, acc: 76%]\n",
      "Valid [Epoch 3/300] [Batch 383/635] [loss: 0.524657, acc: 76%]\n",
      "Valid [Epoch 3/300] [Batch 447/635] [loss: 0.547454, acc: 73%]\n",
      "Valid [Epoch 3/300] [Batch 511/635] [loss: 0.483815, acc: 76%]\n",
      "Valid [Epoch 3/300] [Batch 575/635] [loss: 0.494464, acc: 81%]\n",
      "--------------------------------------------------------------------\n",
      "Train ===> [Epoch 3/300] [Train loss: 0.520568, train acc: 75%]\n",
      "Valid ===> [Epoch 3/300] [Valid loss: 0.527325, valid acc: 73%]\n",
      "--------------------------------------------------------------------\n",
      "Train [Epoch 4/300] [Batch 63/1610] [loss: 0.608919, acc: 73%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-27:\n",
      "Process Process-28:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fd1401f1ba8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 111000) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-20e2a09f7373>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-5f5ebd2e3c55>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, num_epochs, optimizer, criterion, dataloader, model)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mcompare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mcompare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Call Train and Test and save best model\n",
    "'''\n",
    "best_valid_acc = 0.0\n",
    "best_valid_loss = 99999.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(epoch, num_epochs, optimizer, criterion, train_loader, model)\n",
    "    print('------')\n",
    "    valid_loss, valid_acc = test(epoch, num_epochs, criterion, dev_loader, model, 'Valid')\n",
    "    \n",
    "    print('--------------------------------------------------------------------')\n",
    "    print(\"Train ===> [Epoch %d/%d] [Train loss: %f, train acc: %f%%]\" % (epoch, num_epochs, \n",
    "                                                                          train_loss, 100 * train_acc))\n",
    "    print(\"Valid ===> [Epoch %d/%d] [Valid loss: %f, valid acc: %f%%]\" % (epoch, num_epochs, \n",
    "                                                                          valid_loss, 100 * valid_acc))\n",
    "    print('--------------------------------------------------------------------')\n",
    "    \n",
    "    # Save best model\n",
    "    is_best = valid_acc >= best_valid_acc\n",
    "    save_checkpoint({\n",
    "    'epoch': epoch + 1,\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer' : optimizer.state_dict(),\n",
    "    'best_acc' : valid_acc\n",
    "    }, is_best)\n",
    "    \n",
    "    # Plot train/val graphs\n",
    "    train_accuracy.append(train_acc)\n",
    "    val_accuracy.append(valid_acc)\n",
    "    plot_graph(epoch, train_accuracy, val_accuracy, 'Accuracy')\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(valid_loss)\n",
    "    plot_graph(epoch, train_losses, val_losses, 'Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
