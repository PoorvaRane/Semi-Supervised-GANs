{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class make_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data,transform=None):\n",
    "        self.patches = data['X']\n",
    "        self.labels = data['Y']\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        return self.transform(self.patches[idx]),self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Defaults\n",
    "num_epochs = 10 \n",
    "batch_size = 16\n",
    "num_classes = 10 # number of classes for dataset\n",
    "lr = 0.0002 \n",
    "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam: decay of first order momentum of gradient\n",
    "n_cpu = 8 # number of cpu threads to use during batch generation\n",
    "latent_dim = 100 # dimensionality of the latent space\n",
    "img_size = 256 # size of each image dimension\n",
    "channels = 1 # number of output image channels\n",
    "sample_interval = 400 # interval between image sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set cuda\n",
    "if torch.cuda.is_available():\n",
    "    cuda = True \n",
    "else:\n",
    "    cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.use_gpu = cuda\n",
    "        drop_rate = 0.20\n",
    "        num_classes = 2\n",
    "        input_channels = 3\n",
    "        ndf = 16\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=ndf, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(drop_rate),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=ndf, out_channels=ndf*2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(drop_rate),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=ndf*2, out_channels=ndf*4, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(drop_rate),\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=ndf*4, out_channels=ndf*8, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(drop_rate),\n",
    "        )\n",
    "        \n",
    "        # The height and width of downsampled image\n",
    "        ds_size = img_size // 2**4\n",
    "        \n",
    "        # Output layer\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(in_features=128*ds_size**2, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"---------------------------------------\")\n",
    "        #print(\"input size = \", x.size())\n",
    "        x = self.conv1(x)\n",
    "        #print(\"Conv1 output = \", x.size())\n",
    "        x = self.conv2(x)\n",
    "        #print(\"Conv2 output = \", x.size())\n",
    "        x = self.conv3(x)\n",
    "        #print(\"Conv3 output = \", x.size())\n",
    "        x = self.conv4(x)\n",
    "        #print(\"Conv4 output = \", x.size())\n",
    "        \n",
    "        # Flatten the features to give as input to the fc linear classification layer\n",
    "        out = x.view(x.shape[0], -1)\n",
    "        #print(\"Out flatten shape = \", out.size())\n",
    "        out = self.linear1(out)\n",
    "        #print(\"Linear output = \", out.size())\n",
    "        #print(\"---------------------------------------\")\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss and model\n",
    "loss = nn.BCELoss()\n",
    "model = Model()\n",
    "\n",
    "if cuda:\n",
    "    loss.cuda()\n",
    "    model = nn.DataParallel(model)\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "model.apply(weights_init_normal)\n",
    "optimizer = optimizer = torch.optim.Adam(model.parameters(), lr=lr,betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "dataset_path = '../dataset/training_data.npz'\n",
    "data = np.load(dataset_path)\n",
    "transformations = transforms.Compose([transforms.ToTensor(),transforms.Normalize((.5, .5, .5), (.5, .5, .5))])\n",
    "training_data = make_dataset(data,transformations)\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer,criterion,dataloader,model):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "            imgs = Variable(imgs.float())\n",
    "            \n",
    "            labels = Variable(labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print(i)\n",
    "    \n",
    "    return np.mean(epoch_loss)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Loss: 4.7806\n",
      "Epoch [2], Loss: 3.3269\n",
      "Epoch [3], Loss: 2.8183\n",
      "Epoch [4], Loss: 1.5490\n",
      "Epoch [5], Loss: 1.1461\n",
      "Epoch [6], Loss: 0.7695\n",
      "Epoch [7], Loss: 0.6078\n",
      "Epoch [8], Loss: 0.3612\n",
      "Epoch [9], Loss: 0.1618\n",
      "Epoch [10], Loss: 0.3235\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    #train model\n",
    "    epoch_loss = train(optimizer,loss,train_loader,model)\n",
    "    #validation\n",
    "    \n",
    "    #test\n",
    "    \n",
    "    print('Epoch [%d], Loss: %.4f'% (epoch + 1,epoch_loss))\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
