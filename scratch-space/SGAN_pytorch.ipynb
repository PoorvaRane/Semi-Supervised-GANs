{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.utils import save_image\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "num_channels = 1\n",
    "num_classes = 10\n",
    "latent_size = 100\n",
    "labeled_rate = 0.1\n",
    "num_epochs = 1000\n",
    "image_size = 28\n",
    "batch_size = 32\n",
    "epsilon = 1e-8 # used to avoid NAN loss\n",
    "\n",
    "log_path = './SSL_GAN_log.csv'\n",
    "model_path ='./SSL_GAN_model.ckpt'\n",
    "\n",
    "os.makedirs('images', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './torch_data/MNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "class MnistDataset(Dataset):\n",
    "    def __init__(self, image_size, split):\n",
    "        self.split = split\n",
    "        self.mnist_dataset = self._create_dataset(image_size, split)\n",
    "        self.label_mask = self._create_label_mask()\n",
    "        \n",
    "    def _create_dataset(self, image_size, split):\n",
    "        compose = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "        ])\n",
    "        out_dir = '{}/dataset'.format(DATA_FOLDER)\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            return datasets.MNIST(root=out_dir, train=True, transform=compose, download=True)\n",
    "        else:\n",
    "            return datasets.MNIST(root=out_dir, train=False, transform=compose, download=True)\n",
    "        \n",
    "    def _one_hot(self, y):\n",
    "        label = y.item()\n",
    "        label_onehot = np.zeros(num_classes + 1)\n",
    "        label_onehot[label] = 1\n",
    "        return label_onehot\n",
    "    \n",
    "    def _create_label_mask(self):\n",
    "        if self.split == 'train':\n",
    "            l = len(self.mnist_dataset)\n",
    "            label_mask = np.zeros(l)\n",
    "            masked_len = int(labeled_rate * l)\n",
    "            label_mask[0:masked_len] = 1\n",
    "            np.random.shuffle(label_mask)\n",
    "            label_mask = torch.LongTensor(label_mask)\n",
    "            return label_mask\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.mnist_dataset.__getitem__(idx)\n",
    "        label_onehot = self._one_hot(label)\n",
    "        if self.split == 'train':\n",
    "            return data, label, label_onehot, self.label_mask[idx]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloaders\n",
    "def get_loader(image_size, batch_size):\n",
    "    num_workers = 2\n",
    "\n",
    "    mnist_train = MnistDataset(image_size=image_size, split='train')\n",
    "    mnist_test = MnistDataset(image_size=image_size, split='test')\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=mnist_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=mnist_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "        \n",
    "        dropout_rate = 0.25\n",
    "        d = 16\n",
    "        \n",
    "        # Conv operations\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=d, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(dropout_rate)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=d, out_channels=d*2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=d*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=d*2, out_channels=d*4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=d*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(dropout_rate)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=d*4, out_channels=d*8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Linear \n",
    "        self.linear = nn.Linear(in_features=d*8, out_features=(num_classes + 1))\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convolutional Operations\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        # Linear\n",
    "        flatten = x.view(x.size(0), -1)\n",
    "        linear = self.linear(flatten)\n",
    "        prob = self.softmax(linear)\n",
    "        return flatten, linear, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "        \n",
    "        dropout_rate = 0.25\n",
    "        d = 16\n",
    "        \n",
    "        # Conv operations\n",
    "        self.deconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=latent_size, out_channels=d*8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(d*8),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.deconv2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=d*8, out_channels=d*4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=d*4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.deconv3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=d*4, out_channels=d*2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(num_features=d*2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.deconv4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=d*2, out_channels=num_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, x.size(1), 1, 1)\n",
    "        \n",
    "        # Deconvolutional Operations\n",
    "        x = self.deconv1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.deconv3(x)\n",
    "        x = self.deconv4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(size):\n",
    "    n = Variable(torch.randn(size, 100))\n",
    "    if torch.cuda.is_available(): \n",
    "        return n.cuda() \n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "lr = 0.0001 \n",
    "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam: decay of first order momentum of gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "discriminator = DiscriminatorNet()\n",
    "generator = GeneratorNet()\n",
    "\n",
    "# Data Loader\n",
    "train_loader, test_loader = get_loader(image_size, batch_size)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Loss\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "cross_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    discriminator = discriminator.cuda()\n",
    "    generator = generator.cuda()\n",
    "    bce_loss = bce_loss.cuda()\n",
    "    cross_loss = cross_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Data\n",
    "def plot_fake_data(data, grid_size = [5, 5]):\n",
    "    _, axes = plt.subplots(figsize = grid_size, nrows = grid_size[0], ncols = grid_size[1],\n",
    "                           sharey = True, sharex = True)\n",
    "\n",
    "    size = grid_size[0] * grid_size[1]\n",
    "    index = np.int_(np.random.uniform(0, data.shape[0], size = (size)))\n",
    "\n",
    "    figs = data[index].reshape(-1, image_size, image_size)\n",
    "\n",
    "    for idx, ax in enumerate(axes.flatten()):\n",
    "        ax.axis('off')\n",
    "        ax.imshow(figs[idx], cmap = 'gray')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 179/1875] [D loss: 1.691180, acc: 0%] [G loss: 4.582934]\n",
      "[Epoch 0/100] [Batch 359/1875] [D loss: 1.760767, acc: 0%] [G loss: 4.979400]\n",
      "[Epoch 0/100] [Batch 539/1875] [D loss: 0.530599, acc: 0%] [G loss: 6.058153]\n",
      "[Epoch 0/100] [Batch 719/1875] [D loss: 0.920285, acc: 0%] [G loss: 6.765298]\n",
      "[Epoch 0/100] [Batch 899/1875] [D loss: 0.599273, acc: 0%] [G loss: 6.734715]\n",
      "[Epoch 0/100] [Batch 1079/1875] [D loss: 0.174811, acc: 0%] [G loss: 7.291150]\n",
      "[Epoch 0/100] [Batch 1259/1875] [D loss: 0.779838, acc: 0%] [G loss: 7.186805]\n",
      "[Epoch 0/100] [Batch 1439/1875] [D loss: 0.680114, acc: 0%] [G loss: 7.935710]\n",
      "[Epoch 0/100] [Batch 1619/1875] [D loss: 0.531067, acc: 0%] [G loss: 7.502097]\n",
      "[Epoch 0/100] [Batch 1799/1875] [D loss: 0.243370, acc: 0%] [G loss: 7.256360]\n",
      "--------------------------------------------------------------------\n",
      "===> [Epoch 0/100] [Avg D loss: 0.702048, avg acc: 0%] [Avg G loss: 6.470749]\n",
      "--------------------------------------------------------------------\n",
      "[Epoch 1/100] [Batch 179/1875] [D loss: 0.234899, acc: 0%] [G loss: 8.281844]\n",
      "[Epoch 1/100] [Batch 359/1875] [D loss: 0.370323, acc: 0%] [G loss: 9.028146]\n",
      "[Epoch 1/100] [Batch 539/1875] [D loss: 0.009371, acc: 0%] [G loss: 8.981817]\n",
      "[Epoch 1/100] [Batch 719/1875] [D loss: 0.086620, acc: 0%] [G loss: 8.806416]\n",
      "[Epoch 1/100] [Batch 899/1875] [D loss: 0.001026, acc: 0%] [G loss: 9.181060]\n",
      "[Epoch 1/100] [Batch 1079/1875] [D loss: 0.136793, acc: 0%] [G loss: 8.965300]\n",
      "[Epoch 1/100] [Batch 1259/1875] [D loss: 0.518361, acc: 0%] [G loss: 9.256333]\n",
      "[Epoch 1/100] [Batch 1439/1875] [D loss: 0.021013, acc: 0%] [G loss: 9.860654]\n",
      "[Epoch 1/100] [Batch 1619/1875] [D loss: 0.123815, acc: 0%] [G loss: 9.171379]\n",
      "[Epoch 1/100] [Batch 1799/1875] [D loss: 0.015285, acc: 0%] [G loss: 9.043062]\n",
      "--------------------------------------------------------------------\n",
      "===> [Epoch 1/100] [Avg D loss: 0.322289, avg acc: 0%] [Avg G loss: 9.048313]\n",
      "--------------------------------------------------------------------\n",
      "[Epoch 2/100] [Batch 179/1875] [D loss: 0.015260, acc: 0%] [G loss: 9.411192]\n",
      "[Epoch 2/100] [Batch 359/1875] [D loss: 0.060683, acc: 0%] [G loss: 9.217871]\n",
      "[Epoch 2/100] [Batch 539/1875] [D loss: 0.009863, acc: 0%] [G loss: 9.589966]\n",
      "[Epoch 2/100] [Batch 719/1875] [D loss: 0.214994, acc: 0%] [G loss: 9.489901]\n",
      "[Epoch 2/100] [Batch 899/1875] [D loss: 0.414452, acc: 0%] [G loss: 9.338999]\n",
      "[Epoch 2/100] [Batch 1079/1875] [D loss: 0.528697, acc: 0%] [G loss: 10.430937]\n",
      "[Epoch 2/100] [Batch 1259/1875] [D loss: 0.330486, acc: 0%] [G loss: 10.290031]\n",
      "[Epoch 2/100] [Batch 1439/1875] [D loss: 0.006066, acc: 0%] [G loss: 10.035743]\n",
      "[Epoch 2/100] [Batch 1619/1875] [D loss: 0.008898, acc: 0%] [G loss: 10.312222]\n",
      "[Epoch 2/100] [Batch 1799/1875] [D loss: 0.194755, acc: 0%] [G loss: 9.735250]\n",
      "--------------------------------------------------------------------\n",
      "===> [Epoch 2/100] [Avg D loss: 0.237382, avg acc: 0%] [Avg G loss: 9.908115]\n",
      "--------------------------------------------------------------------\n",
      "[Epoch 3/100] [Batch 179/1875] [D loss: 0.272897, acc: 0%] [G loss: 10.867427]\n",
      "[Epoch 3/100] [Batch 359/1875] [D loss: 0.118691, acc: 0%] [G loss: 10.171556]\n",
      "[Epoch 3/100] [Batch 539/1875] [D loss: 0.229009, acc: 0%] [G loss: 10.733997]\n",
      "[Epoch 3/100] [Batch 719/1875] [D loss: 0.121851, acc: 0%] [G loss: 10.290595]\n",
      "[Epoch 3/100] [Batch 899/1875] [D loss: 0.011451, acc: 0%] [G loss: 10.354164]\n",
      "[Epoch 3/100] [Batch 1079/1875] [D loss: 0.379254, acc: 0%] [G loss: 10.860291]\n",
      "[Epoch 3/100] [Batch 1259/1875] [D loss: 0.151550, acc: 0%] [G loss: 11.030635]\n",
      "[Epoch 3/100] [Batch 1439/1875] [D loss: 4.454216, acc: 0%] [G loss: 11.471263]\n",
      "[Epoch 3/100] [Batch 1619/1875] [D loss: 0.143929, acc: 0%] [G loss: 11.210652]\n",
      "[Epoch 3/100] [Batch 1799/1875] [D loss: 0.056795, acc: 0%] [G loss: 10.937548]\n",
      "--------------------------------------------------------------------\n",
      "===> [Epoch 3/100] [Avg D loss: 0.192071, avg acc: 0%] [Avg G loss: 10.677033]\n",
      "--------------------------------------------------------------------\n",
      "[Epoch 4/100] [Batch 179/1875] [D loss: 0.053788, acc: 100%] [G loss: 11.076508]\n",
      "[Epoch 4/100] [Batch 359/1875] [D loss: 0.010907, acc: 0%] [G loss: 11.993075]\n",
      "[Epoch 4/100] [Batch 539/1875] [D loss: 0.001036, acc: 0%] [G loss: 11.397081]\n",
      "[Epoch 4/100] [Batch 719/1875] [D loss: 0.581391, acc: 0%] [G loss: 11.536695]\n",
      "[Epoch 4/100] [Batch 899/1875] [D loss: 0.213279, acc: 100%] [G loss: 11.520294]\n",
      "[Epoch 4/100] [Batch 1079/1875] [D loss: 0.065977, acc: 0%] [G loss: 11.381392]\n",
      "[Epoch 4/100] [Batch 1259/1875] [D loss: 0.025397, acc: 0%] [G loss: 12.399969]\n",
      "[Epoch 4/100] [Batch 1439/1875] [D loss: 0.009377, acc: 100%] [G loss: 12.178127]\n",
      "[Epoch 4/100] [Batch 1619/1875] [D loss: 0.019651, acc: 0%] [G loss: 11.627335]\n",
      "[Epoch 4/100] [Batch 1799/1875] [D loss: 0.001691, acc: 0%] [G loss: 12.644980]\n",
      "--------------------------------------------------------------------\n",
      "===> [Epoch 4/100] [Avg D loss: 0.164145, avg acc: 0%] [Avg G loss: 11.788685]\n",
      "--------------------------------------------------------------------\n",
      "[Epoch 5/100] [Batch 179/1875] [D loss: 0.008626, acc: 0%] [G loss: 12.258383]\n",
      "[Epoch 5/100] [Batch 359/1875] [D loss: 0.263291, acc: 100%] [G loss: 11.801148]\n",
      "[Epoch 5/100] [Batch 539/1875] [D loss: 0.000263, acc: 0%] [G loss: 12.216198]\n",
      "[Epoch 5/100] [Batch 719/1875] [D loss: 0.000572, acc: 100%] [G loss: 12.933918]\n",
      "[Epoch 5/100] [Batch 899/1875] [D loss: 0.002822, acc: 0%] [G loss: 12.355802]\n",
      "[Epoch 5/100] [Batch 1079/1875] [D loss: 0.008465, acc: 100%] [G loss: 12.918288]\n",
      "[Epoch 5/100] [Batch 1259/1875] [D loss: 0.840055, acc: 0%] [G loss: 13.130426]\n",
      "[Epoch 5/100] [Batch 1439/1875] [D loss: 0.000869, acc: 100%] [G loss: 12.963612]\n",
      "[Epoch 5/100] [Batch 1619/1875] [D loss: 0.001892, acc: 0%] [G loss: 12.883550]\n",
      "[Epoch 5/100] [Batch 1799/1875] [D loss: 0.085439, acc: 0%] [G loss: 13.131107]\n",
      "--------------------------------------------------------------------\n",
      "===> [Epoch 5/100] [Avg D loss: 0.140352, avg acc: 0%] [Avg G loss: 12.935970]\n",
      "--------------------------------------------------------------------\n",
      "[Epoch 6/100] [Batch 179/1875] [D loss: 0.032171, acc: 100%] [G loss: 13.029991]\n",
      "[Epoch 6/100] [Batch 359/1875] [D loss: 0.011113, acc: 100%] [G loss: 13.108630]\n",
      "[Epoch 6/100] [Batch 539/1875] [D loss: 0.077046, acc: 0%] [G loss: 13.355167]\n",
      "[Epoch 6/100] [Batch 719/1875] [D loss: 0.001074, acc: 0%] [G loss: 13.090476]\n",
      "[Epoch 6/100] [Batch 899/1875] [D loss: 0.018777, acc: 100%] [G loss: 13.362576]\n",
      "[Epoch 6/100] [Batch 1079/1875] [D loss: 0.011420, acc: 0%] [G loss: 13.839391]\n",
      "[Epoch 6/100] [Batch 1259/1875] [D loss: 0.147760, acc: 100%] [G loss: 13.406661]\n",
      "[Epoch 6/100] [Batch 1439/1875] [D loss: 0.212011, acc: 0%] [G loss: 13.419370]\n",
      "[Epoch 6/100] [Batch 1619/1875] [D loss: 0.005497, acc: 0%] [G loss: 12.997404]\n",
      "[Epoch 6/100] [Batch 1799/1875] [D loss: 0.004767, acc: 100%] [G loss: 13.845627]\n",
      "--------------------------------------------------------------------\n",
      "===> [Epoch 6/100] [Avg D loss: 0.120143, avg acc: 0%] [Avg G loss: 13.434731]\n",
      "--------------------------------------------------------------------\n",
      "[Epoch 7/100] [Batch 179/1875] [D loss: 0.298776, acc: 0%] [G loss: 13.890587]\n",
      "[Epoch 7/100] [Batch 359/1875] [D loss: 0.087187, acc: 0%] [G loss: 14.147421]\n",
      "[Epoch 7/100] [Batch 539/1875] [D loss: 0.140042, acc: 0%] [G loss: 14.701807]\n",
      "[Epoch 7/100] [Batch 719/1875] [D loss: 0.539656, acc: 0%] [G loss: 14.203659]\n",
      "[Epoch 7/100] [Batch 899/1875] [D loss: 0.015439, acc: 0%] [G loss: 14.736726]\n",
      "[Epoch 7/100] [Batch 1079/1875] [D loss: 0.022370, acc: 0%] [G loss: 13.926346]\n",
      "[Epoch 7/100] [Batch 1259/1875] [D loss: 0.000267, acc: 0%] [G loss: 14.808700]\n",
      "[Epoch 7/100] [Batch 1439/1875] [D loss: 0.000257, acc: 100%] [G loss: 15.948786]\n",
      "[Epoch 7/100] [Batch 1619/1875] [D loss: 0.002443, acc: 0%] [G loss: 14.541563]\n",
      "[Epoch 7/100] [Batch 1799/1875] [D loss: 0.132097, acc: 0%] [G loss: 14.936828]\n",
      "--------------------------------------------------------------------\n",
      "===> [Epoch 7/100] [Avg D loss: 0.100711, avg acc: 0%] [Avg G loss: 14.489106]\n",
      "--------------------------------------------------------------------\n",
      "[Epoch 8/100] [Batch 179/1875] [D loss: 0.099170, acc: 0%] [G loss: 14.153547]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-26:\n",
      "Process Process-25:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"<ipython-input-18-02ee4e876d9d>\", line 39, in __getitem__\n",
      "    data, label = self.mnist_dataset.__getitem__(idx)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/mnist.py\", line 74, in __getitem__\n",
      "    img = Image.fromarray(img.numpy(), mode='L')\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/PIL/Image.py\", line 2482, in fromarray\n",
      "    return frombuffer(mode, size, obj, \"raw\", rawmode, 0, 1)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/PIL/Image.py\", line 2428, in frombuffer\n",
      "    im = new(mode, (1, 1))\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/PIL/Image.py\", line 2331, in new\n",
      "    return Image()._new(core.fill(mode, size, color))\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-241ac2107358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Discriminator outputs for real and fake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0md_real_flatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_real_linear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_real_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0md_fake_flatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_fake_linear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_fake_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-89688f6aec6b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout2d\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdropout2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatureDropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/_functions/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, input, p, train, inplace)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Start Training\n",
    "'''\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    correct_epoch = 0\n",
    "    total = 0\n",
    "    G_loss = 0\n",
    "    D_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        img, label, label_onehot, label_mask = data\n",
    "        label_mask = label_mask.float()\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "            label_onehot = label_onehot.cuda()\n",
    "            label_mask = label_mask.cuda()\n",
    "            \n",
    "        b_size = img.size(0)\n",
    "        \n",
    "        # Generate Fake Image\n",
    "        z = noise(b_size)\n",
    "        fake_img = generator(z)\n",
    "        \n",
    "        # Discriminator outputs for real and fake\n",
    "        d_real_flatten, d_real_linear, d_real_prob = discriminator(img.detach())\n",
    "        d_fake_flatten, d_fake_linear, d_fake_prob = discriminator(fake_img)\n",
    "        \n",
    "        ################### Discriminator ####################\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Supervised Loss\n",
    "        supervised_loss = cross_loss(d_real_linear, label)\n",
    "#         d_class_loss_entropy = - torch.sum(label_onehot.float() * torch.log(d_real_prob), dim=1)\n",
    "                \n",
    "        masked_supervised_loss = torch.mul(label_mask, supervised_loss)\n",
    "        delim = torch.Tensor([1.0])\n",
    "        if torch.cuda.is_available():\n",
    "            delim = delim.cuda()\n",
    "        mask_sum = torch.max(delim, torch.sum(label_mask))\n",
    "        d_class_loss = torch.sum(label_mask * masked_supervised_loss) / mask_sum\n",
    "        \n",
    "        # Unsupervised (GAN) Loss\n",
    "        # data is real\n",
    "        prob_real_is_real = 1.0 - d_real_prob[:, -1] + epsilon\n",
    "        tmp_log = torch.log(prob_real_is_real)\n",
    "        d_real_loss = -1.0 * torch.mean(tmp_log)\n",
    "\n",
    "        # data is fake\n",
    "        prob_fake_is_fake = d_fake_prob[:, -1] + epsilon\n",
    "        tmp_log = torch.log(prob_fake_is_fake)\n",
    "        d_fake_loss = -1.0 * torch.mean(tmp_log)\n",
    "\n",
    "        d_loss = d_class_loss + d_real_loss + d_fake_loss\n",
    "\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        \n",
    "        ################### Generator ####################\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # fake data is mistaken to be real\n",
    "        prob_fake_is_real = 1.0 - d_fake_prob[:, -1] + epsilon\n",
    "        tmp_log =  torch.log(prob_fake_is_real)\n",
    "        g_fake_loss = -1.0 * torch.mean(tmp_log)\n",
    "\n",
    "        # Feature Maching\n",
    "        tmp1 = torch.mean(d_real_flatten, dim = 0)\n",
    "        tmp2 = torch.mean(d_fake_flatten, dim = 0)\n",
    "        diff = tmp1 - tmp2\n",
    "        g_feature_loss = torch.mean(torch.mul(diff, diff))\n",
    "\n",
    "        g_loss = g_fake_loss + g_feature_loss\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Accuracy\n",
    "        _, predicted = torch.max(d_real_prob[:, :-1], dim=1)\n",
    "        correct_batch = torch.sum(torch.eq(predicted, label))\n",
    "        batch_accuracy = correct_batch/float(b_size)\n",
    "        \n",
    "        correct_epoch += correct_batch\n",
    "        total += b_size\n",
    "        D_loss += d_loss.item()\n",
    "        G_loss += g_loss.item()\n",
    "        \n",
    "#         if i%180 == 179:\n",
    "#             print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %d%%] [G loss: %f]\" % (epoch, num_epochs, i, \n",
    "#                                        len(train_loader), d_loss.item(), 100 * batch_accuracy, g_loss.item()))\n",
    "\n",
    "        \n",
    "    # Print Epoch results\n",
    "    total_accuracy = correct_epoch/float(total)\n",
    "    avg_D_loss = D_loss/float(i)\n",
    "    avg_G_loss = G_loss/float(i)\n",
    "    \n",
    "    print('--------------------------------------------------------------------')\n",
    "    print(\"===> [Epoch %d/%d] [Avg D loss: %f, avg acc: %d%%] [Avg G loss: %f]\" % (epoch, num_epochs, \n",
    "                                                        avg_D_loss, 100 * total_accuracy, avg_G_loss))\n",
    "    print('--------------------------------------------------------------------')\n",
    "    \n",
    "    # Save Images\n",
    "    save_image(fake_img[:25], 'images/epoch_%d_batch_%d.png' % (epoch, i), nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
